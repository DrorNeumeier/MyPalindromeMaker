{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a7471dfa07644e790038a79243621a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ace4eb4f8eec4e4ab2556420a175d9a1",
              "IPY_MODEL_2550c8cadf424051a6fa52cce5e30abe",
              "IPY_MODEL_6df8e7d12f1e4b7fa4e2c8cd5b3a0998"
            ],
            "layout": "IPY_MODEL_326f360003e94d94b7fe2363b03c4d50"
          }
        },
        "ace4eb4f8eec4e4ab2556420a175d9a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58cde95019cf444d8731bfaf390aedc5",
            "placeholder": "​",
            "style": "IPY_MODEL_f7358e7505654f1198f14ac87bff71e3",
            "value": "Downloading: 100%"
          }
        },
        "2550c8cadf424051a6fa52cce5e30abe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a067eba1fd784fefa00d96bfbbf9d704",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_752b73e768e14c259c86b44bcfa2556a",
            "value": 440473133
          }
        },
        "6df8e7d12f1e4b7fa4e2c8cd5b3a0998": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_908794ae794a4ad6bfbd97f097055dd9",
            "placeholder": "​",
            "style": "IPY_MODEL_d8fbcc96a8ec44d895b5f32651d76841",
            "value": " 440M/440M [00:07&lt;00:00, 61.3MB/s]"
          }
        },
        "326f360003e94d94b7fe2363b03c4d50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58cde95019cf444d8731bfaf390aedc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7358e7505654f1198f14ac87bff71e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a067eba1fd784fefa00d96bfbbf9d704": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "752b73e768e14c259c86b44bcfa2556a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "908794ae794a4ad6bfbd97f097055dd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8fbcc96a8ec44d895b5f32651d76841": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrorNeumeier/MyPalindromeMaker/blob/main/Drors_Palindromes_Maker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Come join me in creating palindromes"
      ],
      "metadata": {
        "id": "lN_2fOwOov84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step1: setup\n",
        "Run the cell below by clicking the Play button on the code block.\n",
        "This step gets the code from github and installs any required libraries. It will take a minute or two"
      ],
      "metadata": {
        "id": "qDLyl05xlrta"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgVQuFpcoZNl",
        "outputId": "144da8eb-27c3-45a6-e663-ee095a12d5d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n",
            "Loading BERT tokenizer...\n",
            "setting up the GPU devices that we will use in training later\n",
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n",
            "\n",
            "\n",
            "\n",
            "Continue to the next step...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if not os.path.exists(\"MyPalindromeMaker\"):\n",
        "  !git clone https://github.com/DrorNeumeier/MyPalindromeMaker\n",
        "else:\n",
        "  !git -C MyPalindromeMaker pull \n",
        "\n",
        "#!git -C MyPalindromeMaker lfs pull\n",
        "#!unzip MyPalindromeMaker/\\*.zip \n",
        "\n",
        "!pip install transformers\n",
        "!pip install wget\n",
        "\n",
        "\n",
        "import random \n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel, BertConfig\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "import shutil\n",
        "import wget\n",
        "\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "print(\"setting up the GPU devices that we will use in training later\")\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    print(\"##################################################################\")\n",
        "    print(\"Can't find a GPU. You REALLY want a GPU for this.\")\n",
        "    print(\"Go click on the little arrow on the top right next to the RAM and Disk monitors\")\n",
        "    print(\"Select View Resources\")\n",
        "    print(\"See if a GPU shows up. I bet you it does not.\")\n",
        "    print(\"Now click on 'change runtime type' at the bottom\")\n",
        "    print(\"Under Hardware accelerator make sure you select a GPU\")\n",
        "    print(\"Try running this cell again\")\n",
        "    print(\"##################################################################\")\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"\\n\\n\\nContinue to the next step...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step2: The palindrome maker\n",
        "Run the cell below by clicking the Play button on the code block.\n",
        "This step builds my code to make palindromes.\n"
      ],
      "metadata": {
        "id": "qnUVQyKgmDqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import numpy as np\n",
        "\n",
        "def loadWords(filename):\n",
        "  f=open(filename)\n",
        "  words=f.readlines()\n",
        "  words=[word.strip().lower() for word in words]\n",
        "  backwardWords = [word[::-1] for word in words]\n",
        "\n",
        "  return words, backwardWords\n",
        "\n",
        "def loadSentences(filename):\n",
        "  f=open(filename)\n",
        "  sentences=f.readlines()\n",
        "  sentences=[sentence.strip().lower() for sentence in sentences]\n",
        "\n",
        "  return sentences\n",
        "\n",
        "\n",
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
        "\n",
        "def makeDictionary(wordList,currentString):\n",
        "\n",
        "  #print(\"makeDictionary\", currentString)\n",
        "\n",
        "  ret={}\n",
        "  ret[\"letters\"] = {}\n",
        "\n",
        "  letters = []\n",
        "  newWordList=[]\n",
        "\n",
        "\n",
        "  for word in wordList:\n",
        "    if(word.startswith(currentString) and len(word)>len(currentString)):\n",
        "      \n",
        "      nextLetter=word[len(currentString)]\n",
        "      newWordList.append(word)\n",
        "      \n",
        "      if nextLetter not in letters:\n",
        "        letters.append(nextLetter)\n",
        "\n",
        "  for letter in letters:\n",
        "    ret[\"letters\"][letter]=makeDictionary(newWordList, currentString+letter)\n",
        "\n",
        "  if(currentString in wordList):\n",
        "    ret[\"letters\"][\" \"]={}\n",
        "\n",
        "  return ret\n",
        "\n",
        "def getDictionaryNode(dictionary, currentString):\n",
        "  if len(currentString) == 0:\n",
        "    return dictionary\n",
        "\n",
        "  letter = currentString[0]\n",
        "  currentString = currentString[1:]\n",
        "\n",
        "  if letter not in dictionary[\"letters\"].keys():\n",
        "    return None\n",
        "\n",
        "  return getDictionaryNode(dictionary[\"letters\"][letter], currentString)\n",
        "\n",
        "\n",
        "def makePalindrome(forwardDictionary, backwardDictionary, forwardString, backwardString, forwardList, backwardList, numberOfWords, printDebug=False):\n",
        "  if printDebug:\n",
        "    print(forwardList, forwardString,\"...\", backwardString, backwardList)\n",
        "  \n",
        "  forwardNode=getDictionaryNode(forwardDictionary, forwardString)\n",
        "  backwardNode=getDictionaryNode(backwardDictionary, backwardString[::-1])\n",
        "\n",
        "  if(len(forwardList)+len(backwardList) == numberOfWords-1 and (getDictionaryNode(forwardDictionary, forwardString+backwardString) != None and \" \" in getDictionaryNode(forwardDictionary, forwardString+backwardString)[\"letters\"].keys())):\n",
        "    forwardList.append(forwardString+backwardString)\n",
        "    forwardString=\"\"\n",
        "    backwardString=\"\"\n",
        "    return forwardList+backwardList\n",
        "\n",
        "\n",
        "  if(random.choice(list(forwardNode[\"letters\"].keys()))==\" \"):\n",
        "    forwardList.append(forwardString)\n",
        "    forwardString=\"\"\n",
        "    forwardNode=forwardDictionary\n",
        "\n",
        "  if(random.choice(list(backwardNode[\"letters\"].keys()))==\" \"):\n",
        "    backwardList.insert(0,backwardString)\n",
        "    backwardString=\"\"\n",
        "    backwardNode=backwardDictionary\n",
        "\n",
        "  if(len(forwardList)+len(backwardList) == numberOfWords and len(backwardString)==0 and len(forwardString)==0):\n",
        "    return forwardList+backwardList\n",
        "\n",
        "  if (len(forwardList)+len(backwardList) > numberOfWords):\n",
        "      return None\n",
        "  \n",
        "  possibleLetters=(set(forwardNode[\"letters\"].keys()) & set(backwardNode[\"letters\"].keys())) - set(\" \")\n",
        "  possibleLetters = list(possibleLetters)\n",
        "\n",
        "  #print(forwardNode[\"letters\"].keys(), backwardNode[\"letters\"].keys())\n",
        "  #print(possibleLetters)\n",
        "\n",
        "  if(len(possibleLetters)==0):\n",
        "    return None\n",
        "\n",
        "  \n",
        "  random.shuffle(possibleLetters)\n",
        "  for letter in possibleLetters:\n",
        "    forwardString_=forwardString+letter\n",
        "    backwardString_=letter+backwardString\n",
        "    forwardList_= forwardList.copy()\n",
        "    backwardList_=backwardList.copy()\n",
        "\n",
        "    ret = makePalindrome(forwardDictionary, backwardDictionary, forwardString_, backwardString_, forwardList_, backwardList_, numberOfWords, printDebug=printDebug)\n",
        "    if(ret):\n",
        "      return ret\n",
        "\n",
        "def getLanguageStats(words, sentences, printDebug=False):\n",
        "\n",
        "  #how long are the sentences\n",
        "  sentenceLenghts = []\n",
        "  for sentence in sentences:\n",
        "    wordCount = sentence.count(\" \")\n",
        "    sentenceLenghts.append(wordCount)\n",
        "\n",
        "  if printDebug:\n",
        "    print(\"Average sentence length\", sum(sentenceLenghts)/len(sentenceLenghts))\n",
        "    print(\"Sentence length histogram\")\n",
        "    counts, bins = np.histogram(sentenceLenghts)\n",
        "    plt.hist(bins[:-1], bins, weights=counts)\n",
        "    plt.show()\n",
        "\n",
        "  sentenceLenghtsCounts = {}\n",
        "  for wordCount in sentenceLenghts:\n",
        "\n",
        "    if wordCount not in sentenceLenghtsCounts.keys():\n",
        "      sentenceLenghtsCounts[wordCount] = 0\n",
        "  \n",
        "    sentenceLenghtsCounts[wordCount] = sentenceLenghtsCounts[wordCount] + 1\n",
        "\n",
        "\n",
        "  #how long are the words in the dictionary\n",
        "  dictionaryWordLengths = []\n",
        "  for word in words:\n",
        "    dictionaryWordLengths.append(len(word))\n",
        "  \n",
        "  if printDebug:\n",
        "    print(\"Average dictionary word length\", sum(dictionaryWordLengths)/len(dictionaryWordLengths))\n",
        "    print(\"Dictonary word length histogram\")\n",
        "    counts, bins = np.histogram(dictionaryWordLengths)\n",
        "    plt.hist(bins[:-1], bins, weights=counts)\n",
        "    plt.show()\n",
        "\n",
        "  dictionaryWordLengthCounts = {}\n",
        "  for letterCount in dictionaryWordLengths:\n",
        "    if letterCount <= 0:\n",
        "      continue\n",
        "    if letterCount not in dictionaryWordLengthCounts.keys():\n",
        "      dictionaryWordLengthCounts[letterCount] = 0\n",
        "\n",
        "    dictionaryWordLengthCounts[letterCount] = dictionaryWordLengthCounts[letterCount] + 1\n",
        "\n",
        "\n",
        "  #how long are the words in the sentences\n",
        "  sentencesWordLengths = []\n",
        "  for sentence in sentences:\n",
        "    words = sentence.split(\" \")\n",
        "    for word in words:\n",
        "      sentencesWordLengths.append(len(word))\n",
        "    \n",
        "  if printDebug:\n",
        "    print(\"Average sentence word length\", sum(sentencesWordLengths)/len(sentencesWordLengths))\n",
        "    print(\"Sentence word length histogram\")\n",
        "    counts, bins = np.histogram(sentencesWordLengths)\n",
        "    plt.hist(bins[:-1], bins, weights=counts)\n",
        "    plt.show()\n",
        "\n",
        "  sentenceWordLengthCounts = {}\n",
        "  for letterCount in sentencesWordLengths:\n",
        "    if letterCount <= 0:\n",
        "      continue\n",
        "    if letterCount not in sentenceWordLengthCounts.keys():\n",
        "      sentenceWordLengthCounts[letterCount] = 0\n",
        "\n",
        "    sentenceWordLengthCounts[letterCount] = sentenceWordLengthCounts[letterCount] + 1\n",
        "\n",
        "  '''\n",
        "  #how likely is each word to show up based on its length\n",
        "  wordProbabilities = {}\n",
        "  totalWordsInSentences = len(sentencesWordLengths)\n",
        "  for word in words:\n",
        "    numberOfWordsOfThisLength = sentenceWordLengthCounts[len(word)]\n",
        "    probabilityOfAWordOfThisLength = numberOfWordsOfThisLength / totalWordsInSentences\n",
        "\n",
        "    wordProbabilities[word] = probabilityOfAWordOfThisLength\n",
        "  '''\n",
        "\n",
        "  return sentenceLenghtsCounts, dictionaryWordLengthCounts, sentenceWordLengthCounts\n",
        "\n",
        "def makeGibberish(N, words, sentences):\n",
        "\n",
        "  ret = []\n",
        "\n",
        "  #for easy access\n",
        "  wordsByLength = {}\n",
        "  for word in words:\n",
        "    if len(word) not in wordsByLength.keys():\n",
        "      wordsByLength[len(word)] = []\n",
        "    \n",
        "    wordsByLength[len(word)].append(word)\n",
        "\n",
        "\n",
        "  #first lets get some stats on the language\n",
        "    \n",
        "  sentenceLenghtsCounts, dictionaryWordLengthCounts, sentenceWordLengthCounts = getLanguageStats(words, sentences)\n",
        "\n",
        "  #just in case there is some crazy words in sentences with crazy lengths that are not in the dictionary\n",
        "  badWordLengths = [l for l in sentenceWordLengthCounts.keys() if l not in wordsByLength.keys()]\n",
        "  for badWordLength in badWordLengths:\n",
        "    del sentenceWordLengthCounts[badWordLength]\n",
        "  #print(\"badWordLengths\", badWordLengths)\n",
        "\n",
        "  retSentenceLengths = random.choices(list(sentenceLenghtsCounts.keys()), weights=list(sentenceLenghtsCounts.values()), k=N)\n",
        "\n",
        "  #print(retSentenceLengths)\n",
        "\n",
        "  for sentenceLength in retSentenceLengths:\n",
        "\n",
        "    sentenceWordLengths = random.choices(list(sentenceWordLengthCounts.keys()), weights=list(sentenceWordLengthCounts.values()), k=sentenceLength)\n",
        "\n",
        "    #print(sentenceWordLengths)\n",
        "\n",
        "    sentence = []\n",
        "    for wordLength in sentenceWordLengths:\n",
        "      word = random.choice(wordsByLength[wordLength])\n",
        "      #print(word)\n",
        "      sentence.append(word)\n",
        "\n",
        "    ret.append(\" \".join(sentence))\n",
        "  \n",
        "  return ret\n",
        "\n",
        "\n",
        "\n",
        "###################################################################\n",
        "### This is code to train the NLP model\n",
        "### Most of this code comes from various tutorials\n",
        "###################################################################\n",
        "\n",
        "def tokenizeSentences(tokenizer, sentences):\n",
        "  inputIds = []\n",
        "  attentionMasks = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for sentence in sentences:\n",
        "      # `encode_plus` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      #   (5) Pad or truncate the sentence to `max_length`\n",
        "      #   (6) Create attention masks for [PAD] tokens.\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          sentence,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                          truncation=True,\n",
        "                          max_length = 64,           # Pad & truncate all sentences.\n",
        "                          pad_to_max_length = True,\n",
        "                          #padding=True,\n",
        "                          return_attention_mask = True,   # Construct attn. masks.\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "        \n",
        "      #The tokenized IDs\n",
        "      inputIds.append(encoded_dict['input_ids'])\n",
        "      #A mask with zeros past the end of the sentence \n",
        "      attentionMasks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  return inputIds, attentionMasks\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "    \n",
        "print(\"Continue to the next step...\")\n"
      ],
      "metadata": {
        "id": "f0LrfxRusEDc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cdcc8ea-bc64-4e5e-a51b-508cd7d6cceb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Continue to the next step...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 3: Choose your dictionary and language reference\n",
        "Select a dictionary from the drop downs to the right and run the cell below by clicking the Play button on the code block.<br>\n",
        "This will load the selected dictionary."
      ],
      "metadata": {
        "id": "q5HujeN5msBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "from urllib.error import HTTPError\n",
        "\n",
        "dictionaryName = 'English (short dictionary):eng' #@param [\"English (short dictionary):eng\", \"English (scrabble dictionary):eng2\", \"King James:kjv\", \"Sci Fi:sci\", \"Shakespeare:sks\", \"Wikipedia:wiki\", \"Jewish Bible:jps\"]\n",
        "dictionaryPrefix = dictionaryName.split(\":\")[1]\n",
        "\n",
        "words, backwardWords = loadWords(\"MyPalindromeMaker/dictionaries/%s_words.txt\" % dictionaryPrefix)\n",
        "\n",
        "sentences = loadSentences(\"MyPalindromeMaker/dictionaries/%s_sentences.txt\" % dictionaryPrefix)\n",
        "\n",
        "print(\"this dictionary has\", len(words), \"many words in it\")\n",
        "print(\"this dictionary has\", len(sentences), \"sentences supporting it\")\n",
        "\n",
        "sentenceLenghtsCounts, dictionaryWordLengthCounts, sentenceWordLengthCounts = getLanguageStats(words, sentences, printDebug=True)\n",
        "\n",
        "print(\"loading dictionary\", dictionaryName, \"this can take a minute ro so\")\n",
        "\n",
        "forwardDictionary=makeDictionary(words, \"\")\n",
        "backwardDictionary=makeDictionary(backwardWords, \"\")\n",
        "\n",
        "print(\"dictionary is loaded.\")\n",
        "\n",
        "if not os.path.exists(\"model_\" + dictionaryPrefix):\n",
        "  print(\"looking for pre-trained model. this can take a minute or so\")\n",
        "  try:\n",
        "    wget.download(\"https://palindromes.s3.amazonaws.com/model_\" + dictionaryPrefix + \".zip\", './model_' + dictionaryPrefix + \".zip\")\n",
        "  \n",
        "    !unzip -o model_\"$dictionaryPrefix\".zip\n",
        "    !rm model_\"$dictionaryPrefix\".zip\n",
        "  \n",
        "  except HTTPError:\n",
        "    print(\"can't download pre-trained model. But don't worry. You can train it yourself in steps 4 and 5\")\n",
        "\n",
        "print(\"\\n\\n\\nContinue to the next step...\")"
      ],
      "metadata": {
        "id": "c1DF03XonI-J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c950c675-1d8b-4840-99b4-36b19e23a843"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this dictionary has 58761 many words in it\n",
            "this dictionary has 6023 sentences supporting it\n",
            "Average sentence length 6.65615141955836\n",
            "Sentence length histogram\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average dictionary word length 8.313626384847092\n",
            "Dictonary word length histogram\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sentence word length 4.283325743282806\n",
            "Sentence word length histogram\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading dictionary English (short dictionary):eng this can take a minute ro so\n",
            "dictionary is loaded.\n",
            "\n",
            "\n",
            "\n",
            "Continue to the next step...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lets try to make a palindrome just for fun"
      ],
      "metadata": {
        "id": "dNwIvmzE_Svh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pick how many words the palindrome should have at random (based on how long sentences are in the languge)\n",
        "howManyWordsInThePalindrome = random.choices(list(sentenceLenghtsCounts.keys()), weights=list(sentenceLenghtsCounts.values()), k=1)[0]\n",
        "\n",
        "palindrome = makePalindrome(forwardDictionary, backwardDictionary, \"\", \"\", [], [], howManyWordsInThePalindrome, printDebug=True)\n",
        "\n",
        "print(\" \".join(palindrome))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2Ec1z1T_YAF",
        "outputId": "9af06e2e-21d4-4292-b172-c28c581aadb0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]  ...  []\n",
            "[] x ... x []\n",
            "[] xa ... ax []\n",
            "[] xe ... ex []\n",
            "[] xen ... nex []\n",
            "[] xer ... rex []\n",
            "[] xy ... yx []\n",
            "[] xr ... rx []\n",
            "[] xra ... arx []\n",
            "[] r ... r []\n",
            "[] ro ... or []\n",
            "[] roi ... ior []\n",
            "[] rois ... sior []\n",
            "[] rou ... uor []\n",
            "[] roug ... guor []\n",
            "[] roul ... luor []\n",
            "[] ros ... sor []\n",
            "[] rosi ... isor []\n",
            "[] rod ... dor []\n",
            "[] rodn ... ndor []\n",
            "[] rodne ... endor []\n",
            "[] ron ... nor []\n",
            "[] rona ... anor []\n",
            "[] rob ... bor []\n",
            "[] rom ... mor []\n",
            "[] rome ... emor []\n",
            "[] romu ... umor []\n",
            "[] rov ... vor []\n",
            "[] rovi ... ivor []\n",
            "[] roo ... oor []\n",
            "[] room ... moor []\n",
            "[] rooms ... s ['moor']\n",
            "['rooms'] m ... ms ['moor']\n",
            "['rooms'] my ... yms ['moor']\n",
            "['rooms'] myn ... nyms ['moor']\n",
            "['rooms'] ms ... sms ['moor']\n",
            "['rooms', 'ms'] a ... asms ['moor']\n",
            "['rooms', 'ms'] al ... lasms ['moor']\n",
            "['rooms', 'ms'] alp ... plasms ['moor']\n",
            "['rooms', 'ms'] ai ... iasms ['moor']\n",
            "['rooms', 'ms'] ais ... siasms ['moor']\n",
            "['rooms', 'ms'] ac ... casms ['moor']\n",
            "['rooms', 'ms'] acr ... rcasms ['moor']\n",
            "['rooms', 'ms'] ap ... pasms ['moor']\n",
            "['rooms', 'ms'] aps ... spasms ['moor']\n",
            "['rooms', 'ms'] apse ... e ['spasms', 'moor']\n",
            "['rooms', 'ms'] apses ... se ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] i ... ise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] id ... dise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ida ... adise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] idi ... idise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] il ... lise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ili ... ilise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ile ... elise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ill ... llise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'ill'] a ... allise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'ill'] at ... tallise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'ill'] i ... illise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] im ... mise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'im'] e ... emise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'im'] eh ... hemise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'im'] et ... temise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'im'] eti ... itemise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'im'] ed ... demise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'im'] er ... remise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'im'] i ... imise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'im'] in ... nimise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'im'] ini ... inimise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'im'] inim ... minimise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'im'] it ... timise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'im'] iti ... itimise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'im'] itc ... ctimise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'im'] o ... omise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'im'] ot ... tomise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'im'] od ... domise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'im'] odo ... odomise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'im'] or ... romise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'im'] orp ... promise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'im'] on ... nomise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'im'] r ... rmise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] io ... oise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ion ... noise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ions ... s ['noise', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] iono ... o ['noise', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ionos ... so ['noise', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ionosp ... pso ['noise', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ioni ... i ['noise', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ionis ... si ['noise', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ionisa ... asi ['noise', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ionic ... ci ['noise', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ionia ... ai ['noise', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ionian ... nai ['noise', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] iot ... toise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] it ... tise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ita ... atise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] its ... stise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'its'] a ... astise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'its'] ah ... hastise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] iti ... itise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] itin ... nitise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ite ... etise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] itc ... ctise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] is ... sise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ia ... aise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ir ... rise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] iro ... orise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] iri ... irise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ire ... erise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ira ... arise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ig ... gise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] in ... nise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ing ... gnise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ingo ... ognise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ini ... inise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] init ... tinise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ino ... onise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] inor ... ronise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] inu ... unise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] inn ... nnise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] inna ... annise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] innar ... rannise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ine ... enise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ina ... anise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] inab ... banise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] inac ... canise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] inr ... rnise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] iv ... vise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ivo ... ovise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ivor ... rovise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ive ... evise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'ive'] d ... devise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'ive'] r ... revise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'ive'] l ... levise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'ive'] le ... elevise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'ive'] let ... televise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ip ... pise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ic ... cise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ice ... ecise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'ice'] r ... recise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ica ... acise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] icar ... racise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ici ... icise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] icil ... licise ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] u ... use ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ur ... ruse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ure ... eruse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] urt ... truse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] um ... muse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] umb ... b ['muse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] umbr ... rb ['muse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] umbra ... arb ['muse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] umbrag ... garb ['muse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] umbre ... erb ['muse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] umbi ... ib ['muse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] umbil ... lib ['muse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] uml ... l ['muse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] umla ... al ['muse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] umlau ... ual ['muse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] umlaut ... tual ['muse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ump ... p ['muse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] umpi ... ip ['muse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] umpir ... rip ['muse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] umps ... sp ['muse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] us ... suse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] usi ... isuse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] uf ... fuse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ufo ... ofuse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'ufo'] r ... rofuse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ub ... buse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ul ... luse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ulc ... cluse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ulce ... ecluse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ulcer ... recluse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ulcera ... a ['recluse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ulcerat ... ta ['recluse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ulcerate ... eta ['recluse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ulcerati ... ita ['recluse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ulcero ... o ['recluse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ulcerou ... uo ['recluse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ulcers ... s ['recluse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ut ... tuse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] uh ... huse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] l ... lse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] le ... else ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leu ... u ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leuk ... ku ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] let ... t ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lets ... st ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lett ... tt ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] letti ... itt ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lettu ... utt ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lette ... ett ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leth ... ht ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leg ... g ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] legr ... rg ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] legro ... org ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] legg ... gg ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] legge ... egg ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] legu ... ug ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] legum ... mug ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lege ... eg ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leger ... reg ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lega ... ag ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] legat ... tag ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] legal ... lag ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] legi ... ig ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] legib ... big ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] les ... s ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leso ... os ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lesot ... tos ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lesi ... is ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lesio ... ois ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lesion ... nois ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] less ... ss ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lesso ... oss ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lessor ... ross ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lesse ... ess ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lesser ... ress ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lessen ... ness ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lesseni ... iness ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lessenin ... niness ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lessens ... sness ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lessene ... eness ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lessened ... deness ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lesb ... bs ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lesbi ... ibs ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lest ... ts ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lesl ... ls ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lesli ... ils ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leslie ... eils ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leb ... b ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lem ... m ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lemu ... um ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lemur ... rum ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lemo ... om ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lemon ... nom ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lemm ... mm ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lemmi ... imm ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ley ... y ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leyd ... dy ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leyde ... edy ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leyden ... nedy ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lec ... c ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lex ... x ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lexe ... ex ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lexi ... ix ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] len ... n ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lend ... dn ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lenn ... nn ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lenny ... ynn ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leng ... gn ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leni ... in ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lenie ... ein ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lenin ... nin ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lei ... i ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leip ... pi ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leig ... gi ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leis ... si ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leit ... ti ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lee ... e ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leec ... ce ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leek ... ke ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leew ... we ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leewa ... awe ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lees ... se ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leer ... re ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leed ... de ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] led ... d ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lep ... p ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lepr ... rp ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lepre ... erp ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lepe ... ep ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leper ... rep ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lea ... a ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lead ... da ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leadi ... ida ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leade ... eda ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leav ... va ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leavi ... iva ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leave ... eva ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leaven ... neva ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leag ... ga ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leah ... ha ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leas ... sa ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lease ... esa ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leasi ... isa ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leat ... ta ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leak ... ka ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leaka ... aka ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leaki ... ika ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leake ... eka ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leaks ... ska ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leac ... ca ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lean ... na ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leann ... nna ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leanne ... enna ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leane ... ena ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leaner ... rena ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leani ... ina ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leanin ... nina ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leant ... tna ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leaf ... fa ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leafl ... lfa ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leap ... pa ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lear ... ra ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lew ... w ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lev ... v ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leve ... ev ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lever ... rev ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] levi ... iv ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] levit ... tiv ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leo ... o ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leot ... to ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leota ... ato ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leotar ... rato ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leon ... no ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leona ... ano ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leone ... eno ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leop ... po ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] leopa ... apo ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lef ... f ['else', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] la ... alse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lu ... ulse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] lup ... pulse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] p ... pse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] po ... opse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] poc ... copse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] pock ... k ['copse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] pocks ... sk ['copse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] pocke ... ek ['copse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] py ... ypse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] pyl ... lypse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] pm ... mpse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'pm'] i ... impse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'pm'] il ... limpse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] pi ... ipse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] pil ... lipse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] pilc ... clipse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] pill ... llipse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] pa ... apse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] pal ... lapse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] pall ... llapse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] pallo ... ollapse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] pale ... elapse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] pales ... s ['elapse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] palef ... f ['elapse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] palefa ... af ['elapse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] palefac ... caf ['elapse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] paleface ... ecaf ['elapse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] palel ... l ['elapse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] palely ... yl ['elapse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] palen ... n ['elapse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] palene ... en ['elapse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] palenes ... sen ['elapse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] paleness ... ssen ['elapse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] paler ... r ['elapse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] paled ... d ['elapse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] palet ... t ['elapse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] palett ... tt ['elapse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] palette ... ett ['elapse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] pan ... napse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] pr ... rpse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] pro ... orpse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] proc ... corpse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] procl ... l ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] procla ... al ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] proclam ... mal ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] proclai ... ial ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] proclaim ... mial ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] procli ... il ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] procliv ... vil ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] proclivi ... ivil ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] proct ... t ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] procto ... ot ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] proctor ... rot ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] procu ... u ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] procur ... ru ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] procure ... eru ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] procures ... seru ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] procr ... r ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] procre ... er ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] procra ... ar ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] procras ... sar ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] procrast ... tsar ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] proce ... e ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] proced ... de ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] procedu ... ude ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] procedur ... rude ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] proces ... se ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] process ... sse ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] processe ... esse ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] processa ... asse ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] processo ... osse ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] processor ... rosse ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] procee ... ee ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] proceed ... dee ['corpse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] t ... tse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] te ... etse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] tes ... setse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] test ... tsetse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testa ... a ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testam ... ma ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testame ... ema ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testamen ... nema ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testab ... ba ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] tests ... s ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] teste ... e ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] tester ... re ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testes ... se ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] tested ... de ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testt ... t ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testtu ... ut ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testtub ... but ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testy ... y ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testi ... i ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testic ... ci ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testin ... ni ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testim ... mi ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testimo ... omi ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testis ... si ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testil ... li ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testily ... yli ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testif ... fi ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testifi ... ifi ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testie ... ei ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testd ... d ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testdr ... rd ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testdri ... ird ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testo ... o ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] testos ... so ['tsetse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] r ... rse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] re ... erse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rem ... merse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rep ... perse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] reps ... sperse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'reps'] i ... isperse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'reps'] id ... disperse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'reps'] r ... rsperse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'reps'] re ... ersperse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'reps'] ret ... tersperse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ret ... terse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retc ... c ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retu ... u ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retur ... ru ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retun ... nu ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retune ... enu ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retr ... r ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retri ... ir ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retrie ... eir ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retria ... air ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retrial ... lair ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retre ... er ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retren ... ner ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retry ... yr ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retra ... ar ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retral ... lar ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retrac ... car ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retraci ... icar ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retrace ... ecar ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retrai ... iar ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retran ... nar ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retro ... or ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retrov ... vor ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retrovi ... ivor ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retrog ... gor ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retros ... sor ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retrof ... for ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] reth ... h ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retho ... oh ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] reta ... a ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retai ... ia ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retail ... lia ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retain ... nia ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retains ... snia ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retaine ... enia ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retainer ... renia ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retaini ... inia ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retak ... ka ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retaki ... ika ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retake ... eka ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retar ... ra ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retard ... dra ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retal ... la ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rete ... e ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] reten ... ne ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retel ... le ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retell ... lle ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retes ... se ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retest ... tse ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] reteste ... etse ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rets ... s ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retsi ... is ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retsin ... nis ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retsina ... anis ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rety ... y ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retyp ... py ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retypi ... ipy ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retype ... epy ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] reto ... o ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retol ... lo ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retou ... uo ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retor ... ro ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retort ... tro ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retorte ... etro ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retorts ... stro ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retoo ... oo ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retook ... koo ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rett ... t ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rette ... et ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retted ... det ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] reti ... i ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retir ... ri ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retire ... eri ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retit ... ti ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retie ... ei ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retin ... ni ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retini ... ini ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retinit ... tini ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retina ... ani ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retinu ... uni ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] retic ... ci ['terse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rev ... verse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] reve ... e ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revei ... ie ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] reveil ... lie ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] reveill ... llie ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revea ... ae ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] reveal ... lae ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] reven ... ne ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revena ... ane ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revenan ... nane ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] reveng ... gne ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revenu ... une ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rever ... re ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] reveri ... ire ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] reverie ... eire ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] reverb ... bre ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revert ... tre ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] reverti ... itre ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] reverts ... stre ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] reverte ... etre ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revere ... ere ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revel ... le ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revelr ... rle ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revela ... ale ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revelat ... tale ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revels ... sle ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revell ... lle ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revelli ... ille ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revelle ... elle ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revi ... i ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revis ... si ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revisa ... asi ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revit ... ti ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revita ... ati ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revie ... ei ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revil ... li ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revile ... eli ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revili ... ili ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] reviv ... vi ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revive ... evi ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revs ... s ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revv ... v ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revve ... ev ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revvi ... iv ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revu ... u ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revue ... eu ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revul ... lu ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revo ... o ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revoc ... co ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revoca ... aco ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revol ... lo ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revok ... ko ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] reva ... a ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revam ... ma ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] reval ... la ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revalu ... ula ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revalua ... aula ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] revan ... na ['verse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ru ... urse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rub ... burse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rubs ... sburse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'rubs'] i ... isburse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'rubs'] id ... disburse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ruc ... curse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ruck ... k ['curse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rucks ... sk ['curse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rucksa ... ask ['curse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rucksac ... cask ['curse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ruct ... t ['curse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ructi ... it ['curse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ructio ... oit ['curse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] run ... nurse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] runa ... a ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] runaw ... wa ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rung ... g ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] runs ... s ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] runo ... o ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] runof ... fo ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rund ... d ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rundo ... od ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] runn ... n ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] runny ... yn ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] runne ... en ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] runner ... ren ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] runna ... an ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] runnab ... ban ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] runni ... in ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] runnin ... nin ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] runnie ... ein ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] runnier ... rein ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] runt ... t ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] runts ... st ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rune ... e ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] runes ... se ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] runw ... w ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] runwa ... aw ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] runway ... yaw ['nurse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rup ... purse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rupe ... e ['purse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rupee ... ee ['purse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ruper ... re ['purse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rupert ... tre ['purse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rupt ... t ['purse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ruptu ... ut ['purse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ruptur ... rut ['purse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ro ... orse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rod ... dorse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rodn ... ndorse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rodne ... endorse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rodney ... y ['endorse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rog ... gorse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rogu ... u ['gorse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rogue ... eu ['gorse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] roge ... e ['gorse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] roger ... re ['gorse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rom ... morse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] row ... worse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rowb ... b ['worse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rowbo ... ob ['worse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rowi ... i ['worse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rowin ... ni ['worse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rowe ... e ['worse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rower ... re ['worse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rowed ... de ['worse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rowd ... d ['worse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rowdi ... id ['worse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rowdin ... nid ['worse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rowdil ... lid ['worse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rowdy ... yd ['worse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rows ... s ['worse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] roc ... corse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rock ... k ['corse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rocke ... ek ['corse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rocker ... rek ['corse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rocki ... ik ['corse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rockin ... nik ['corse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rockie ... eik ['corse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rocks ... sk ['corse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rockso ... osk ['corse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rocs ... s ['corse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] roco ... o ['corse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rococ ... co ['corse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rococo ... oco ['corse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ra ... arse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rap ... parse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rapp ... p ['parse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rappe ... ep ['parse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rappo ... op ['parse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rappor ... rop ['parse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rapport ... trop ['parse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rappr ... rp ['parse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rappi ... ip ['parse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rappin ... nip ['parse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rapi ... i ['parse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rapin ... ni ['parse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rapid ... di ['parse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rapidi ... idi ['parse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rapis ... si ['parse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rapie ... ei ['parse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] raps ... s ['parse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rapa ... a ['parse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rapac ... ca ['parse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rapaci ... ica ['parse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rapacit ... tica ['parse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rape ... e ['parse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] rapt ... t ['parse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] y ... yse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] w ... wse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wo ... owse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wor ... rowse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] word ... drowse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wordy ... y ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wordl ... l ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wordle ... el ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wordles ... sel ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wordless ... ssel ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] words ... s ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wordsm ... ms ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wordsmi ... ims ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wordsmit ... tims ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wordp ... p ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wordpl ... lp ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wordpla ... alp ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wordpr ... rp ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wordg ... g ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wordga ... ag ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wordgam ... mag ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] worda ... a ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wordag ... ga ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wordage ... ega ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] worde ... e ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] worded ... de ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wordi ... i ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wordin ... ni ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wordie ... ei ['drowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wod ... dowse ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wodg ... g ['dowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wodge ... eg ['dowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] woda ... a ['dowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] wodan ... na ['dowse', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] a ... ase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] al ... lase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alv ... v ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alve ... ev ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] all ... l ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] allo ... ol ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] allow ... wol ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alloc ... col ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] allot ... tol ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] allots ... stol ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alli ... il ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] allit ... til ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] allie ... eil ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] allies ... seil ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] allia ... ail ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] allian ... nail ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] allig ... gil ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alle ... el ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aller ... rel ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] allerg ... grel ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] allel ... lel ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] allev ... vel ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] allevi ... ivel ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alleg ... gel ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] allu ... ul ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] allus ... sul ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alla ... al ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] allay ... yal ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ally ... yl ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ali ... i ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aliv ... vi ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alive ... evi ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aliz ... zi ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aliza ... azi ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alig ... gi ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] align ... ngi ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aliq ... qi ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alia ... ai ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alias ... sai ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alik ... ki ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alic ... ci ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alib ... bi ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alibi ... ibi ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aliba ... abi ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alie ... ei ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alien ... nei ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alip ... pi ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alin ... ni ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alini ... ini ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alina ... ani ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alis ... si ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aliso ... osi ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alim ... mi ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alimo ... omi ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alime ... emi ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alk ... k ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alka ... ak ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alkal ... lak ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] als ... s ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] also ... os ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alf ... f ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alfa ... af ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alm ... m ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] almi ... im ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alma ... am ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alman ... nam ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] almo ... om ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] almos ... som ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] almon ... nom ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alms ... sm ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alb ... b ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alp ... p ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alpi ... ip ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alpin ... nip ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alpa ... ap ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alpac ... cap ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alps ... sp ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alr ... r ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ale ... e ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aler ... re ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alert ... tre ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alerti ... itre ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alerte ... etre ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alertn ... ntre ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alertne ... entre ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alerts ... stre ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aleh ... he ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alem ... me ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ales ... se ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alex ... xe ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alexa ... axe ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alc ... c ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alco ... oc ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alcov ... voc ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alcoh ... hoc ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alg ... g ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alge ... eg ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] algeb ... beg ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alger ... reg ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] algo ... og ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] algor ... rog ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] algi ... ig ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alga ... ag ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] algal ... lag ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alu ... u ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alum ... mu ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alt ... t ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alw ... w ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alwa ... aw ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alway ... yaw ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alo ... o ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alou ... uo ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aloud ... duo ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aloo ... oo ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aloe ... eo ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alon ... no ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alone ... eno ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aloh ... ho ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aloha ... aho ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alof ... fo ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ala ... a ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ald ... d ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alde ... ed ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aldeh ... hed ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alder ... red ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aldr ... rd ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aldri ... ird ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aly ... y ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alys ... sy ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alyss ... ssy ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] alyssa ... assy ['lase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ab ... base ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abe ... ebase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'abe'] d ... debase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'abe'] m ... mebase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'abe'] mi ... imebase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'abe'] mit ... timebase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aba ... abase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abat ... t ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abatt ... tt ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abatto ... ott ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abattoi ... iott ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abate ... et ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abatem ... met ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abated ... det ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abates ... set ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abaf ... f ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aban ... n ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aband ... dn ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abas ... s ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abase ... es ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abased ... des ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abasem ... mes ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abaseme ... emes ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abasemen ... nemes ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abash ... hs ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abal ... l ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abalo ... ol ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abalon ... nol ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abalone ... enol ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abac ... c ['abase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abl ... lbase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] able ... elbase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'able'] e ... eelbase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'able'] eh ... heelbase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abr ... rbase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] abri ... irbase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ar ... rase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] are ... erase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ac ... case ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] acn ... ncase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] acne ... encase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] act ... tcase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] acti ... itcase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ack ... kcase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] acr ... rcase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] acri ... ircase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] acre ... ercase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ad ... dase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] adi ... idase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] at ... tase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ata ... atase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ae ... ease ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aes ... sease ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aer ... rease ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] av ... vase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ave ... e ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ava ... a ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avan ... na ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avant ... tna ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aval ... la ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avala ... ala ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avai ... ia ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avail ... lia ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] availi ... ilia ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] availa ... alia ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] availe ... elia ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] availed ... delia ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avar ... ra ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avari ... ira ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avo ... o ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avoi ... io ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avoir ... rio ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avoid ... dio ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avoida ... adio ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avoc ... co ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avoca ... aco ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avow ... wo ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avi ... i ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avid ... di ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avia ... ai ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] aviar ... rai ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avian ... nai ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avio ... oi ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avion ... noi ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avu ... u ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avun ... nu ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] avul ... lu ['vase', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] an ... nase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ani ... inase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ap ... pase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] api ... ipase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ah ... hase ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] o ... ose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] on ... nose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] one ... enose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ong ... gnose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ot ... tose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] od ... dose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] or ... rose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ora ... arose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] oral ... l ['arose', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] oran ... n ['arose', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] orang ... gn ['arose', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] orangu ... ugn ['arose', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] orac ... c ['arose', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] oracl ... lc ['arose', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] orat ... t ['arose', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] orate ... et ['arose', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] orated ... det ['arose', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] orates ... set ['arose', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] orato ... ot ['arose', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] orator ... rot ['arose', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] orati ... it ['arose', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] oratin ... nit ['arose', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] oratio ... oit ['arose', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] orp ... prose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] orc ... crose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ore ... erose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ort ... trose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] om ... mose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] oi ... iose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] oo ... oose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] oom ... moose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] oomp ... p ['moose', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] oompa ... ap ['moose', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] oompah ... hap ['moose', 'spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ool ... loose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ooh ... hoose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses', 'ooh'] c ... choose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] oh ... hose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ol ... lose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ob ... bose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] obo ... obose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] oc ... cose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ocu ... ucose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] ocul ... lucose ['spasms', 'moor']\n",
            "['rooms', 'ms', 'apses'] op ... pose ['spasms', 'moor']\n",
            "rooms ms apses oppose spasms moor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Now it is time to train the Natural Lang Processing model\n",
        "We are going to use a Bidirectional Encoder Representations from Transformers (or Bert for short) model.\n",
        "Bert models are good at processing language. You can learn more about them [here](https://www.techtarget.com/searchenterpriseai/definition/BERT-language-model)\n",
        "We will be using one called [BertForSequenceClassification](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification)\n",
        "This Bert model is a classifier. That means it has a layer at the end that classifies the input into 1 of several ... well classifications. In our case we will have 2 classificaitons: 1 and 0. We tag \"good\" sentences from our langudge reference set as a 1 and \"bad\" gibirish sentences as a 0. Hopefully our classifier will learn to tell real sentences from gibirish ones. <br>\n",
        "\n",
        "The next few steps are based on multiple tutorials and we are thankful to the many people who help make ML accessible by publishing thier code. In particular see [BERT-fine-tuning](https://mccormickml.com/2019/07/22/BERT-fine-tuning/) for in depth details on what we are doing here. \n",
        "\n",
        "The trained models were downloaded in the previous steps so you can skip the next 2 steps. But if you want to you can train the model yourself using this cell.<br>"
      ],
      "metadata": {
        "id": "9V0qls6U2Lsx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 4: Setup a training set\n",
        "\n",
        "You can pick the size of the training set to use on the right. We recomand to use 10000.<br>\n",
        "Run the cell below by clicking the Play button on the code block.<br>\n",
        "This will create the training set but will not start the training yet. That happens in the next cell.\n",
        "\n"
      ],
      "metadata": {
        "id": "c-y1iUhJs8Yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rebuildModelIfAlreadyThere = \"yes\" #@param [\"yes\", \"no\"]\n",
        "trainingSetSize = \"10000\" #@param [5000, 10000, 20000]\n",
        "\n",
        "trainingSetSize = int(trainingSetSize)\n",
        "trainingSetSizeHalf = int(trainingSetSize/2)\n",
        "\n",
        "if os.path.exists(\"model_%s\" % dictionaryPrefix ) and rebuildModelIfAlreadyThere == \"no\":\n",
        "  print(\"This model is already trained. You can skip to the next step\")\n",
        "else:\n",
        "\n",
        "  print(\"First we need to create the training set\")\n",
        "\n",
        "  trainingListSentences = random.choices(sentences, k=trainingSetSizeHalf)\n",
        "  trainingListLabels = [1] * (trainingSetSizeHalf)\n",
        "\n",
        "  #print(sentenceLenghts)\n",
        "  generatedSenteceLengths = makeGibberish(trainingSetSizeHalf, words, sentences)\n",
        "\n",
        "  for sentence in generatedSenteceLengths:\n",
        "    #just in case\n",
        "    if True: #if len(sentence) < 64:\n",
        "      trainingListSentences.append(sentence)\n",
        "      trainingListLabels.append(0)\n",
        "\n",
        "  print(\"Here is a real sentence\")\n",
        "  print(\"\\\"\",trainingListSentences[0],\"\\\"\")\n",
        "  print(\"\\nAnd here is a gibirish sentence\")\n",
        "  print(\"\\\"\",trainingListSentences[trainingSetSizeHalf], \"\\\"\")\n",
        "\n",
        "\n",
        "  print(\"\\nand here is the tokenized sentence\")\n",
        "  print('Tokenized: ', tokenizer.tokenize(trainingListSentences[0]))\n",
        "  print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(trainingListSentences[0])))\n",
        "\n",
        "  inputIds, attentionMasks = tokenizeSentences(tokenizer, trainingListSentences)\n",
        "  # Convert the lists into tensors.\n",
        "  inputIds = torch.cat(inputIds, dim=0)\n",
        "  attentionMasks = torch.cat(attentionMasks, dim=0)\n",
        "  trainingListLabels = torch.tensor(trainingListLabels)\n",
        "\n",
        "  # Combine the training inputs into a TensorDataset.\n",
        "  dataset = TensorDataset(inputIds, attentionMasks, trainingListLabels)\n",
        "\n",
        "  # Create a 90-10 train-validation split.\n",
        "  train_size = int(0.9 * len(dataset))\n",
        "  val_size = len(dataset) - train_size\n",
        "  train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "  print('{:>5,} training samples'.format(train_size))\n",
        "  print('{:>5,} validation samples'.format(val_size))\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "  # For fine-tuning BERT it is recommended to use batch size of 16 or 32.\n",
        "  batch_size = 32\n",
        "\n",
        "  # Create the Data and Validation Loaders \n",
        "  train_dataloader = DataLoader(\n",
        "              train_dataset,  # The training samples.\n",
        "              sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "              batch_size = batch_size # Trains with this batch size.\n",
        "          )\n",
        "\n",
        "  validation_dataloader = DataLoader(\n",
        "              val_dataset, # The validation samples.\n",
        "              sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "              batch_size = batch_size # Evaluate with this batch size.\n",
        "          )\n",
        "\n",
        "  #this is the actual model we will be training\n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "      num_labels = 2, # The number of output labels--in our case 2. GOOD/BAD. \n",
        "      output_attentions = False, # Whether the model returns attentions weights.\n",
        "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  )\n",
        "\n",
        "  # Tell pytorch to run this model on the GPU.\n",
        "  model.cuda()\n",
        "\n",
        "  optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "  # Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "  # This is probably too much. We will end training early if it looks like we are overfitting\n",
        "  epochs = 4\n",
        "\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "  # Create the learning rate scheduler.\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "\n",
        " \n",
        "print(\"Continue to next step\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468,
          "referenced_widgets": [
            "0a7471dfa07644e790038a79243621a4",
            "ace4eb4f8eec4e4ab2556420a175d9a1",
            "2550c8cadf424051a6fa52cce5e30abe",
            "6df8e7d12f1e4b7fa4e2c8cd5b3a0998",
            "326f360003e94d94b7fe2363b03c4d50",
            "58cde95019cf444d8731bfaf390aedc5",
            "f7358e7505654f1198f14ac87bff71e3",
            "a067eba1fd784fefa00d96bfbbf9d704",
            "752b73e768e14c259c86b44bcfa2556a",
            "908794ae794a4ad6bfbd97f097055dd9",
            "d8fbcc96a8ec44d895b5f32651d76841"
          ]
        },
        "id": "2EoYPo71pAe9",
        "outputId": "c2ceba6c-6df2-438b-b7f2-7a1cd919b749"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First we need to create the training set\n",
            "Here is a real sentence\n",
            "\" the water melted into ice \"\n",
            "\n",
            "And here is a gibirish sentence\n",
            "\" be whole puns tic sixes ye \"\n",
            "\n",
            "and here is the tokenized sentence\n",
            "Tokenized:  ['the', 'water', 'melted', 'into', 'ice']\n",
            "Token IDs:  [1996, 2300, 12501, 2046, 3256]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9,000 training samples\n",
            "1,000 validation samples\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a7471dfa07644e790038a79243621a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Continue to next step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 5: Do the actual training\n",
        "Run the cell below by clicking the Play button on the code block.<br>\n",
        "This will start the actual training of the model. This will take a few minutes. Go make a cup of coffee"
      ],
      "metadata": {
        "id": "pd20AyNMHov8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(\"model_%s\" % dictionaryPrefix ) and rebuildModelIfAlreadyThere==\"no\":\n",
        "  print(\"This model is already trained. You can skip to the next step\")\n",
        "else:\n",
        "\n",
        "  # Set the seed value to create arbitrary randomness\n",
        "  seed_val = 42\n",
        "\n",
        "  random.seed(seed_val)\n",
        "  np.random.seed(seed_val)\n",
        "  torch.manual_seed(seed_val)\n",
        "  torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "  # We'll store a number of quantities such as training and validation loss, \n",
        "  # validation accuracy, and timings.\n",
        "  training_stats = []\n",
        "\n",
        "  # Measure the total training time for the whole run.\n",
        "  total_t0 = time.time()\n",
        "\n",
        "  avg_val_accuracy = 0\n",
        "\n",
        "  # For each epoch...\n",
        "  for epoch_i in range(0, epochs):\n",
        "\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "\n",
        "      #if epoch_i >= 2 and avg_val_accuracy > 0.99:\n",
        "      #  print(\"Training accuracy is over %99 and it's been 2 epochs. We can call it quits early.\")\n",
        "      #  break\n",
        "\n",
        "      print('Training...')\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      total_train_loss = 0\n",
        "\n",
        "      # Put the model into training mode.\n",
        "      model.train()\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "          # Progress update every 40 batches.\n",
        "          if step % 40 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          # Unpack this training batch from our dataloader and copy to the GPU using the 'to' method\n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a\n",
        "          # backward pass.\n",
        "          model.zero_grad()        \n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          loss, logits = model(b_input_ids, \n",
        "                              token_type_ids=None, \n",
        "                              attention_mask=b_input_mask, \n",
        "                              labels=b_labels, return_dict = False)\n",
        "\n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end.\n",
        "          total_train_loss += loss.item()\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0.\n",
        "          # This is to help prevent the \"exploding gradients\" problem.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters and take a step using the computed gradient.\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          scheduler.step()\n",
        "\n",
        "      # Calculate the average loss over all of the batches.\n",
        "      avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "      \n",
        "      # Measure how long this epoch took.\n",
        "      training_time = format_time(time.time() - t0)\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss: {0:.4f}\".format(avg_train_loss))\n",
        "      print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "          \n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "      # After the completion of each training epoch, measure our performance on\n",
        "      # our validation set.\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Running Validation...\")\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Put the model in evaluation mode\n",
        "      model.eval()\n",
        "\n",
        "      # Tracking variables \n",
        "      total_eval_accuracy = 0\n",
        "      total_eval_loss = 0\n",
        "      nb_eval_steps = 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in validation_dataloader:\n",
        "          \n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "          \n",
        "          # Tell pytorch not to bother with constructing the compute graph during\n",
        "          # the forward pass, since this is only needed for backprop (training).\n",
        "          with torch.no_grad():        \n",
        "\n",
        "              # Forward pass\n",
        "              (loss, logits) = model(b_input_ids, \n",
        "                                    token_type_ids=None, \n",
        "                                    attention_mask=b_input_mask,\n",
        "                                    labels=b_labels, return_dict = False)\n",
        "              \n",
        "          # Accumulate the validation loss.\n",
        "          total_eval_loss += loss.item()\n",
        "\n",
        "          # Move logits and labels to CPU\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "          # Calculate the accuracy for this batch of test sentences, and\n",
        "          # accumulate it over all batches.\n",
        "          total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "          \n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "      print(\"  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n",
        "\n",
        "      # Calculate the average loss over all of the batches.\n",
        "      avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "      \n",
        "      # Measure how long the validation run took.\n",
        "      validation_time = format_time(time.time() - t0)\n",
        "      \n",
        "      print(\"  Validation Loss: {0:.4f}\".format(avg_val_loss))\n",
        "      print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "      # Record all statistics from this epoch.\n",
        "      training_stats.append(\n",
        "          {\n",
        "              'epoch': epoch_i + 1,\n",
        "              'Training Loss': avg_train_loss,\n",
        "              'Valid. Loss': avg_val_loss,\n",
        "              'Valid. Accur.': avg_val_accuracy,\n",
        "              'Training Time': training_time,\n",
        "              'Validation Time': validation_time\n",
        "          }\n",
        "      )\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "\n",
        "  print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "\n",
        "  # Create a DataFrame from our training statistics.\n",
        "  pd.set_option('precision', 2)\n",
        "  df_stats = pd.DataFrame(data=training_stats)\n",
        "  df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "  print(df_stats)\n",
        "\n",
        "  print(\"saving the model\")\n",
        "  model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "  model_to_save.save_pretrained(\"model_%s\" % dictionaryPrefix)\n",
        "  tokenizer.save_pretrained(\"model_%s\" % dictionaryPrefix)\n",
        "\n",
        "print(\"Continue to next step\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZiQkADuPS-9",
        "outputId": "0b86253d-fb3a-4e5e-e174-7817b622ece9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    282.    Elapsed: 0:00:15.\n",
            "  Batch    80  of    282.    Elapsed: 0:00:28.\n",
            "  Batch   120  of    282.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    282.    Elapsed: 0:00:53.\n",
            "  Batch   200  of    282.    Elapsed: 0:01:06.\n",
            "  Batch   240  of    282.    Elapsed: 0:01:20.\n",
            "  Batch   280  of    282.    Elapsed: 0:01:33.\n",
            "\n",
            "  Average training loss: 0.0614\n",
            "  Training epcoh took: 0:01:33\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.9971\n",
            "  Validation Loss: 0.0138\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    282.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    282.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    282.    Elapsed: 0:00:40.\n",
            "  Batch   160  of    282.    Elapsed: 0:00:53.\n",
            "  Batch   200  of    282.    Elapsed: 0:01:07.\n",
            "  Batch   240  of    282.    Elapsed: 0:01:20.\n",
            "  Batch   280  of    282.    Elapsed: 0:01:34.\n",
            "\n",
            "  Average training loss: 0.0029\n",
            "  Training epcoh took: 0:01:34\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.9980\n",
            "  Validation Loss: 0.0143\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    282.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    282.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    282.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    282.    Elapsed: 0:00:55.\n",
            "  Batch   200  of    282.    Elapsed: 0:01:08.\n",
            "  Batch   240  of    282.    Elapsed: 0:01:22.\n",
            "  Batch   280  of    282.    Elapsed: 0:01:36.\n",
            "\n",
            "  Average training loss: 0.0030\n",
            "  Training epcoh took: 0:01:36\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.9980\n",
            "  Validation Loss: 0.0134\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    282.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    282.    Elapsed: 0:00:28.\n",
            "  Batch   120  of    282.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    282.    Elapsed: 0:00:55.\n",
            "  Batch   200  of    282.    Elapsed: 0:01:09.\n",
            "  Batch   240  of    282.    Elapsed: 0:01:22.\n",
            "  Batch   280  of    282.    Elapsed: 0:01:36.\n",
            "\n",
            "  Average training loss: 0.0003\n",
            "  Training epcoh took: 0:01:37\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.9980\n",
            "  Validation Loss: 0.0150\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:06:34 (h:mm:ss)\n",
            "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
            "epoch                                                                         \n",
            "1           6.14e-02         0.01            1.0       0:01:33         0:00:03\n",
            "2           2.86e-03         0.01            1.0       0:01:34         0:00:03\n",
            "3           2.96e-03         0.01            1.0       0:01:36         0:00:04\n",
            "4           2.67e-04         0.02            1.0       0:01:37         0:00:04\n",
            "saving the model\n",
            "Continue to next step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 6: Lets make some palindromes!\n",
        "Ok we have our model trained for our selected dictionary. The model is good at seperating real sentences from gibirish.\n",
        "We can now make a bunch of palindromes and run them through the model to filter our the gibirish. \n",
        "\n",
        "Select the number of words in a palindrome and number of palindromes you want to make from the drop down on the right and run the cell below by clicking the Play button on the code block."
      ],
      "metadata": {
        "id": "UTXT_5tkUGQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numberOfWordsInPalindrome = \"choose for me\" #@param [\"choose for me\", 3,5,6,7,8,9,10,15,20,30]\n",
        "desiredNumberOfPalindromes = \"5\" #@param [1, 5, 10, 20]\n",
        "\n",
        "if numberOfWordsInPalindrome == \"choose for me\":\n",
        "  numberOfWordsInPalindrome = -1\n",
        "numberOfWordsInPalindrome = int(numberOfWordsInPalindrome)\n",
        "desiredNumberOfPalindromes = int(desiredNumberOfPalindromes)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"model_%s\" % dictionaryPrefix)\n",
        "model_loaded = BertForSequenceClassification.from_pretrained(\"model_%s\" % dictionaryPrefix)\n",
        "model_loaded = model_loaded.to(device)\n",
        "\n",
        "palindromesFound = set()\n",
        "\n",
        "print(\"Making palindromes using dictionary:\", dictionaryName)\n",
        "\n",
        "i = 0\n",
        "while len(palindromesFound) < desiredNumberOfPalindromes:\n",
        "\n",
        "  i = i + 1\n",
        "  if i%1000 == 0:\n",
        "    print(\"...I have tried %i palindromes and found %i that pass the language model\" % (i, len(palindromesFound)))\n",
        "\n",
        "  #make a palindrome\n",
        "  if numberOfWordsInPalindrome > 0:\n",
        "    N = int(numberOfWordsInPalindrome)\n",
        "  else:\n",
        "    N = random.choices(list(sentenceLenghtsCounts.keys()), weights=list(sentenceLenghtsCounts.values()), k=1)[0]\n",
        "  \n",
        "  palindrome = makePalindrome(forwardDictionary, backwardDictionary, \"\", \"\", [], [], N)\n",
        "  sentence = \" \".join(palindrome)\n",
        "\n",
        "  #now run it through the model to see if it a sensible sentence\n",
        "  inputIds, attentionMasks = tokenizeSentences(tokenizer, [sentence])\n",
        "  inputId = inputIds.pop()\n",
        "  attentionMask = attentionMasks.pop()\n",
        "\n",
        "  inputId = torch.LongTensor(inputId)\n",
        "  attentionMask = torch.LongTensor(attentionMask)\n",
        "  \n",
        "  inputId = inputId.to(device)\n",
        "  attentionMask = attentionMask.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model_loaded(inputId, token_type_ids=None, attention_mask=attentionMask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  index = logits.argmax()\n",
        "\n",
        "  if index == 1 and sentence not in palindromesFound:\n",
        "      print(sentence)\n",
        "      palindromesFound.add(sentence)\n",
        "  else:\n",
        "      pass\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7dMIo6qUyuy",
        "outputId": "c243ddd5-05d7-45ec-a6b0-1014f7563658"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making palindromes using dictionary: English (short dictionary):eng\n",
            "harasses opposes sarah\n",
            "ports harasses opposes sarah strop\n",
            "hanna vases oppose savannah\n",
            "...I have tried 1000 palindromes and found 3 that pass the language model\n",
            "emu loved iced decide volume\n",
            "tubes oppose but\n"
          ]
        }
      ]
    }
  ]
}